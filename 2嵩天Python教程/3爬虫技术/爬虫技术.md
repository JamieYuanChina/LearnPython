Python 网络爬虫与信息提取

The Website is the API...

Requests库

安装 pip3 install requests

在python环境中测试requests库

```
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8'
>>> r.text
```

 Requests库有7个主要方法

| 方法               | 说明                                          |
| ------------------ | --------------------------------------------- |
| requests.request() | 构造一个请求，支撑一下个方法的基础方法        |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET       |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的HEAD    |
| requests.post()    | 向HTML网页提交POST请求方法，对应于HTTP的POST  |
| requests.put()     | 向HTML网页提交PUT请求方法，对应于HTTP的PUT    |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH |
| requests.delete()  | 向HTML网页提交删除请求，对应于HTTP的DELETE    |

r = requests.get(url)

构造一个向服务器请求资源的Request对象，返回一个包含服务器自愿的额Response对象。

requests.get(url,params=None,**kwargs) 这个是get方法的完整用法，其中url为拟获取页面的url连接，params为url中的额外参数，字典或者字节流格式，可选。 * * kwargs为12个控制访问的参数。

request方法是其他六个方法的基础，其他六个方法都是对于request方法在某一方面应用的再次封装。

Response对象的属性：

| 属性                | 说明                                            |
| ------------------- | ----------------------------------------------- |
| r.status_code       | HTTP请求的返回状态，200表示连接陈宫，其他为失败 |
| r.text              | HTTP相应内容的字符串格式，即url对应的页面内容   |
| r.encoding          | 从HTTP header中猜测的相应内容编码方式           |
| r.apparent_encoding | 从内容中分析出的响应内容编码方式(备选编码方式)  |
| r.content           | HTTP响应内容的二进制形式。                      |

Requests库的异常

| 异常                      | 说明                                        |
| ------------------------- | ------------------------------------------- |
| requests.ConnectionError  | 网络连接错误异常，如DNS查询失败、拒绝连接等 |
| requests.HTTPError        | HTTP错误异常                                |
| requests.URLRequired      | URL缺失异常                                 |
| requests.TooManyRedirects | 超过最大重定向次数，产生重定向异常          |
| requests.ConnectTimeout   | 连接远程服务器超时异常                      |
| requests.Timeout          | 请求URL超时，产生超时异常                   |

r.raise_for_status() 如果不是200，产生异常 requests.HTTPError

异常处理通用框架：

```python
import requests
def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
if __name__ == "__main__"
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

HTTP ，Hypertext Transfer Protocol 超文本传输协议，一个基于请求与响应模式的、无状态的应用层协议。采用URL作为定位网络资源的标识，URL格式如下:

```
http://host[:port][path]
```

host为合法的Internet主机域名或者IP地址；port为端口号，缺省为80；path请求资源的路径。

网络爬虫的尺寸

小规模，数据量小，爬取速度不敏感，主要是爬取网页级别，使用Requests库。

中规模，数据规模较大，爬取速度敏感，主要是爬取网站级别，使用Scrapy库。

大规模，搜索引擎爬取速度关键，主要是爬取全网信息，一般定制开发。

小规模爬虫占到了爬虫应用的90%以上。

网络爬虫的限制有两种方法，一是来源审查，主要是检查请求这的user-agent字段。二是发布公告，也就是robots。

Robots Exclusion Standard 网络爬虫排除标准，作用是网站告知网络爬虫那些页面可以抓取，那些不可以。

形式是在网站根目录下放置一个文本文件，robots.txt。比如

http://www.jd.com/robots.txt

```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

User-agent :那些访问者

Disallow:不允许访问的路径。

如果网站没有robots文件，那么说明允许无限制访问。

正则表达式 regular expression        regex        RE

正则表达式是用来简介表达一组字符串的表达式。

五个网站数据爬取实例：

Beautiful soup 美味汤，主要用来解析HTML和XML

```
pip3 install beautifulsoup4
```

```python
>>> import requests
>>> r = requests.get("http://python123.io/ws/demo.html")
>>> r.text
'<html><head><title>This is a python demo page</title></head>\r\n<body>\r\n<p class="title"><b>The demo python introduces several python courses.</b></p>\r\n<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n<a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1">Basic Python</a> and <a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2">Advanced Python</a>.</p>\r\n</body></html>'
>>> demo = r.text
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(demo, "html.parser")
>>> print(soup.prettify())
```

soup = BeautifulSoup(demo, "html.parser")

解析函数第一个参数为数据，第二个为解析器。

Beautiful Soup库也叫beautifulsoup4，也叫bs4

from bs4 import BeautifulSoup

import bs4

Beautiful Soup库解析器

| 解析器           | 使用方法                        | 条件                 |
| ---------------- | ------------------------------- | -------------------- |
| bs4的HTML解析器  | BeautifulSoup(mk,'html.parser') | 安装bs4库            |
| lxml的HTML解析器 | BeautifulSoup(mk,'lxml')        | pip install lxml     |
| lxml的XML解析器  | BeautifulSoup(mk,'xml')         | pip install lxml     |
| html5lib的解析器 | BeautifulSoup(mk,'html5lib')    | pip install html5lib |

Beautiful Soup类的基本元素

| 基本元素        | 说明                                                         |
| --------------- | ------------------------------------------------------------ |
| Tab             | 标签，最基本的信息组织单元，分别使用<>和</>标明开头和结尾    |
| Name            | 标签的名字，< p >... < /p >的名字是p，格式：< tag >.name     |
| Attributes      | 标签的属性，字典形式组织，格式：< tag >.attrs                |
| NavigableString | 标签内非属性字符串，< >...< / >中的字符串，格式< tag >.string |
| Comment         | 标签内字符串的注释部分，一种搞特殊的Comment类型              |

```
>>> soup.title
<title>This is a python demo page</title>
>>> tag = soup.a
>>> tag
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>
>>> soup.a.name  # a标签的名称
'a'
>>> soup.a.parent.name  # a标签的父标签的名称
'p'
>>> soup.a.parent.parent.name
'body'
>>> soup.a.parent.parent.parent.name
'html'
>>> soup.a.parent.parent.parent.parent.name
'[document]'
>>> tag.attrs  # 标签属性
{'class': ['py1'], 'href': 'http://www.icourse163.org/course/BIT-268001', 'id': 'link1'}
>>> soup.a.string # a标签的字符串
'Basic Python'
>>> 

```

Buautiful Soup中HTML标签的遍历方式

下行遍历

| 属性         | 说明                                                    |
| ------------ | ------------------------------------------------------- |
| .contents    | 子节点的列表，将< tag >所有儿子节点存入列表             |
| .children    | 子节点的迭代类型，与.contents类型，用于循环遍历儿子节点 |
| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历      |

```
for child in soup.body.chinlren  # 遍历儿子节点
    print(child)
```

上行遍历

| 属性     | 说明                                           |
| -------- | ---------------------------------------------- |
| .parent  | 节点的父亲标签                                 |
| .parents | 节点的先辈标签的迭代类型，用于循环遍历先辈节点 |

```python
for parent in soup.a.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)
```

平行遍历

| 属性               | 说明                                                 |
| ------------------ | ---------------------------------------------------- |
| .next_sibling      | 返回按照HTML文本顺序的下一个平行节点标签             |
| .previous_sibling  | 返回按照HTML文本顺序的上一个平行节点标签             |
| .next_siblings     | 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 |
| .previous_siblings | 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 |

标签输的平行遍历必须是在同一个父亲节点下的各个节点。

在标签树中，平行遍历的下一个节点不一定是标签类型。

prettify()方法，在每一个标签后面增加了换行符号，这样打印出来的html文本更加友好和清晰。

bs4库把所有读入的文本都转换为了utf-8编码。

信息标记的三中形式

信息的标记

标记后的信息可形成信息组织结构，增加了信息维度，可用于通信、存储或展示，标记结构与信息一样有重要价值，有利于程序或人的利用和理解。

HTML(hyper text markup language)是www(World Wide Web)的信息组织方式。

能将声音，图像，视频，嵌入到文本中。HTML通过预定义的< >...< / >标签形式组织不同类型的信息。

信息标记目前国际工人的有三中形式分别是：XML、JSON，YAML

XML  extensible markup language

通过标签形式构建信息。注释为 <!----->

JSON  JavaScript Object Notation,

有类型的键值对 key:value  键值中字符串需要双引号，值可以有多个，使用“name":[“name1”,“name2”]，键值对可以嵌套使用，使用大括号{}表示

```
"key":"value"
"key":["value1","value2"]
"key":{"subkey":"subvalue"}
```

YAML YAML Ain't Markup Language

无类型的键值对组织数据，通过缩进表示所属关系，类似python，# 表示注释

XML实例

```xml
<person>
    <firstName>Jamie</firstName>
    <lastName>Yuan</lastName>
    <address>
        <streetAddr>槐安西路</streetAddr>
        <city>石家庄市</city>
        <zipcode>050000</zipcode>
    </address>
</person>
```

JSON实例

```json
{
    "firstName":"Jamie",
    "lastNAme" :"Yuan",
    "address"  :{
                  "streetAddr":"槐安西路"
        		  “city”      :"石家庄市"
        		  “zipcode”   :"050000"
                },
    "prof"     :["Computer System","Security"]
}
```

YAML实例

```yaml
firstName:Jamie
LastName :Yuan
address  :
    streetAddr:槐安西路
    city      :石家庄市
    zipcode   :050000
prof     :
-Computer System
-Security
```

三中信息标记形式的比较

XML最早的通用信息标记语言，可扩展性号，但是繁琐

JSON信息有类型，适合程序处理(js)，较XML简洁

YAML信息无类型，文本信息比例最高，可读性好

Internet上的信息交互主要使用XML

JSON在移动应用云端和节点的信息通信，无注释

YAML各类系统的配置文件，有注释易读

信息提取的一般方法：

方法一：完整解析信息的标记形式，在提取关键信息，需要标记解析器，例如bs4库的标签树遍历，优点是信息解析准确，缺点是提取过程繁琐，速度慢。

方法二：无视标记形式，直接搜索关键信息。对信息文本查找函数即可，优点是提取过程简洁，速度快，缺点是提取结果准确性与信息内容相关。

最好的方法是融合两种方法。

```python
>>> for link in soup.find_all('a'):
...     print(link.get('href'))
... 
http://www.icourse163.org/course/BIT-268001
http://www.icourse163.org/course/BIT-1001870001
```

<>.find_all(name, attrs,recursive,string,**kwargs)

返回一个列表类型，存储查找的结果。

name：对标签名称的检索字符串。

```python
soup.find_all('a')
soup.find_all(['a','b'])
for tag in soup.find_all(True):
    print(tag.name)
    
>>> import re
>>> for tag in soup.find_all(re.compile('b')):
...     print(tag.name)
```

attrs:对标签属性值的检索字符串，可标注属性检索。

```python
soup.find_all('p', 'course')
soup.find_all(id='link1')
import re
soup.find_all(id=re.compile('link'))
```

recursive:是否最子孙全部检索，默认为True。

```python
>>> soup.find_all('a')
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]
>>> soup.find_all('a', recursive=False)
[]
```

string:<>...</>标签中间的字符串域的检索字符串。

```python
>>> soup.find_all(string = "Basic Python")
['Basic Python']
>>> soup.find_all(string = re.compile("python"))
['This is a python demo page', 'The demo python introduces several python courses.']
```

由于find_all函数非常常用，所以存在一种简写形式

< tag >(..) 等价于 < tag >.fine_all(..)

soup(..) 等价于 soup.fine_all(..)

Beautiful Soup库的find_all方法有7个扩展方法：

| 方法                        | 说明                                                    |
| --------------------------- | ------------------------------------------------------- |
| <>.find()                   | 搜索且只返回一个结果，字符串类型，同.find_all()参数     |
| <>.find_parents()           | 在先辈节点中搜索，返回列表类型，同.find_all()参数       |
| <>.find_parent()            | 在先辈节点中返回一个结果，字符串类型，同.find()参数     |
| <>.find_next_siblings()     | 在后续平行节点中搜索，返回列表类型，同.find_all()参数   |
| <>.find_next_sibling()      | 在后续平行节点中返回一个结果，字符串类型，同.find()参数 |
| <>.find_previous_siblings() | 在前序平行节点中搜索，返回列表类型，同.find_all()参数   |
| <>.find_previous_sibling()  | 在前序平行节点中返回一个结果，字符串类型，同.find()参数 |

实例：中国大学排名实例

网址：

http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html

定向爬虫：仅对输入URL进行爬取，不扩展爬取。

程序见CrawUnivRanking.py

字符串匹配主要使用正则表达式。正则表达式的使用需要编译，也就是将符合正则表达式语法的字符串转换成正则表达式特征。

正则表达式语法，正则表达式是由字符和操作符构成。

```
P(Y|YT|YTH|YTHO)?N
```

正则表达式的常用操作符

| 操作符 | 说明                             | 实例                                    |
| ------ | -------------------------------- | --------------------------------------- |
| .      | 表示任何单个字符                 |                                         |
| []     | 字符集，对单个字符给出取值范围   | [abc]表示a、b、c，[a-z]表示a到z单个字符 |
| [^]    | 非字符集，对单个字符给出排除范围 | [^abc]表示非a或b或c的单个字符           |
| *      | 前一个字符0次或无限次扩展        | abc*表示ab,abc,abcc,abccc等             |
| +      | 前一个字符1次或无限次扩展        | abc+表示abc，abcc，abccc等              |
| ?      | 前一个字符0次或1次扩展           | abc？表示ab,bac                         |
| \|     | 左右表达式任意一个               | abc\|def表示abc、def                    |
| {m}    | 扩展前一个字符m次                | ab{2}c表示abbc                          |
| {m,n}  | 扩展前一个字符m至n次             | ab{1,2}c表示abc、abbc                   |
| ^      | 匹配字符串开头                   | ^abc表示abc且在一个字符串的开头         |
| $      | 匹配字符串结尾                   | abc$表示abc且在一个字符串的结尾         |
| ()     | 分组标记，内部只能使用\|操作符   | （abc)表示abc,(abc\|def)表示abc、def    |
| \d     | 数字，等价于[0-9]                |                                         |
| \w     | 单词字符，等价于[A-Za-z0-9]      |                                         |

^[A-Za-z]+$  由26个字母组成的字符串

^[A-Za-z0-9]+$  由26个字母和数字组成的字符串

^-?\d+$ 整数形式的字符串

^[0-9]*[1-9] [0-9] *$ 正整数形式的字符

[1-9]\d{5}中国境内邮政编码，6位

[\u4e00-\u9fa5]匹配中文字符

\d{3}-\d{8}|\d{4}-\d{7}  国内电话号码，010-68913536

匹配IP地址的正则表达式

IP地址字符串形式的正则表达式

\d+.\d+.\d+.\d+

\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}

以上两个表达式都不能精确的表示IP地址字符串。要想精确表达，需要分段考虑。

0-99：[1-9]?\d    100-199:1\d{2}  200-249:2[0-4]\d  250-255:25[0-5]

(([1-9]?\d|1\d{2}|2[0-4]\d|25[0-5]).){3}([1-9]?\d|1\d{2}|2[0-4]\d|25[0-5])

Re库是Python的标准库，主要用于字符串匹配，import re即可。

原生字符串raw string也就是不包含转义字符的字符串

r'[1-9]\d{5}'  

在python中\被解析为转义符号，如果需要使用\符号，需要 \ \表示。比较繁琐。这就引入了原生字符串。

Re库的主要功能函数

| 函数          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| re.search()   | 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象  |
| re.match()    | 从一个字符串开始位置匹配正则表达式，返回match对象            |
| re.findall()  | 搜索字符串，以列表类型返回全部能匹配的字符子串               |
| re.split()    | 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型     |
| re.finditer() | 搜索字符串，反馈已给匹配结果的迭代类型，每个迭代元素是match对象 |
| re.sub()      | 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 |

re.search(pattern, string,flags=0)

pattern:正则表达式字符串或原生字符串表示

string： 待匹配的字符串

flags:正则表达式使用时的一些标记

| 常用标记            | 说明                                                         |
| ------------------- | ------------------------------------------------------------ |
| re.l  re.IGNORECASE | 忽略正则表达式的大小写                                       |
| re.M re.MULTILINE   | 正则表达式中的^操作符能将给定字符串的每一行当做配屏开始      |
| re.S re.DOTALL      | 正则表达式中的.操作符匹配所有字符串，默认匹配除了换行外的所有字符 |

例子：

```python
>>> import re
>>> match = re.search(r'[1-9]\d{5}', 'BIT 100081')
>>> if match:
...     print(match.group(0))

```

38 08:22