Python 网络爬虫与信息提取

The Website is the API...

Requests库

安装 pip3 install requests

在python环境中测试requests库

```
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8'
>>> r.text
```

 Requests库有7个主要方法

| 方法               | 说明                                          |
| ------------------ | --------------------------------------------- |
| requests.request() | 构造一个请求，支撑一下个方法的基础方法        |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET       |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的HEAD    |
| requests.post()    | 向HTML网页提交POST请求方法，对应于HTTP的POST  |
| requests.put()     | 向HTML网页提交PUT请求方法，对应于HTTP的PUT    |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH |
| requests.delete()  | 向HTML网页提交删除请求，对应于HTTP的DELETE    |

r = requests.get(url)

构造一个向服务器请求资源的Request对象，返回一个包含服务器自愿的额Response对象。

requests.get(url,params=None,**kwargs) 这个是get方法的完整用法，其中url为拟获取页面的url连接，params为url中的额外参数，字典或者字节流格式，可选。 * * kwargs为12个控制访问的参数。

request方法是其他六个方法的基础，其他六个方法都是对于request方法在某一方面应用的再次封装。

Response对象的属性：

| 属性                | 说明                                            |
| ------------------- | ----------------------------------------------- |
| r.status_code       | HTTP请求的返回状态，200表示连接陈宫，其他为失败 |
| r.text              | HTTP相应内容的字符串格式，即url对应的页面内容   |
| r.encoding          | 从HTTP header中猜测的相应内容编码方式           |
| r.apparent_encoding | 从内容中分析出的响应内容编码方式(备选编码方式)  |
| r.content           | HTTP响应内容的二进制形式。                      |

Requests库的异常

| 异常                      | 说明                                        |
| ------------------------- | ------------------------------------------- |
| requests.ConnectionError  | 网络连接错误异常，如DNS查询失败、拒绝连接等 |
| requests.HTTPError        | HTTP错误异常                                |
| requests.URLRequired      | URL缺失异常                                 |
| requests.TooManyRedirects | 超过最大重定向次数，产生重定向异常          |
| requests.ConnectTimeout   | 连接远程服务器超时异常                      |
| requests.Timeout          | 请求URL超时，产生超时异常                   |

r.raise_for_status() 如果不是200，产生异常 requests.HTTPError

异常处理通用框架：

```python
import requests
def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
if __name__ == "__main__"
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

HTTP ，Hypertext Transfer Protocol 超文本传输协议，一个基于请求与响应模式的、无状态的应用层协议。采用URL作为定位网络资源的标识，URL格式如下:

```
http://host[:port][path]
```

host为合法的Internet主机域名或者IP地址；port为端口号，缺省为80；path请求资源的路径。

网络爬虫的尺寸

小规模，数据量小，爬取速度不敏感，主要是爬取网页级别，使用Requests库。

中规模，数据规模较大，爬取速度敏感，主要是爬取网站级别，使用Scrapy库。

大规模，搜索引擎爬取速度关键，主要是爬取全网信息，一般定制开发。

小规模爬虫占到了爬虫应用的90%以上。

网络爬虫的限制有两种方法，一是来源审查，主要是检查请求这的user-agent字段。二是发布公告，也就是robots。

Robots Exclusion Standard 网络爬虫排除标准，作用是网站告知网络爬虫那些页面可以抓取，那些不可以。

形式是在网站根目录下放置一个文本文件，robots.txt。比如

http://www.jd.com/robots.txt

```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

User-agent :那些访问者

Disallow:不允许访问的路径。

如果网站没有robots文件，那么说明允许无限制访问。

正则表达式 regular expression        regex        RE

正则表达式是用来简介表达一组字符串的表达式。

五个网站数据爬取实例：

20