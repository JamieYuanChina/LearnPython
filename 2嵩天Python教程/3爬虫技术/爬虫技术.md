Python 网络爬虫与信息提取

The Website is the API...

Requests库

安装 pip3 install requests

在python环境中测试requests库

```
>>> import requests
>>> r = requests.get("http://www.baidu.com")
>>> r.status_code
200
>>> r.encoding = 'utf-8'
>>> r.text
```

 Requests库有7个主要方法

| 方法               | 说明                                          |
| ------------------ | --------------------------------------------- |
| requests.request() | 构造一个请求，支撑一下个方法的基础方法        |
| requests.get()     | 获取HTML网页的主要方法，对应于HTTP的GET       |
| requests.head()    | 获取HTML网页头信息的方法，对应于HTTP的HEAD    |
| requests.post()    | 向HTML网页提交POST请求方法，对应于HTTP的POST  |
| requests.put()     | 向HTML网页提交PUT请求方法，对应于HTTP的PUT    |
| requests.patch()   | 向HTML网页提交局部修改请求，对应于HTTP的PATCH |
| requests.delete()  | 向HTML网页提交删除请求，对应于HTTP的DELETE    |

r = requests.get(url)

构造一个向服务器请求资源的Request对象，返回一个包含服务器自愿的额Response对象。

requests.get(url,params=None,**kwargs) 这个是get方法的完整用法，其中url为拟获取页面的url连接，params为url中的额外参数，字典或者字节流格式，可选。 * * kwargs为12个控制访问的参数。

request方法是其他六个方法的基础，其他六个方法都是对于request方法在某一方面应用的再次封装。

Response对象的属性：

| 属性                | 说明                                            |
| ------------------- | ----------------------------------------------- |
| r.status_code       | HTTP请求的返回状态，200表示连接陈宫，其他为失败 |
| r.text              | HTTP相应内容的字符串格式，即url对应的页面内容   |
| r.encoding          | 从HTTP header中猜测的相应内容编码方式           |
| r.apparent_encoding | 从内容中分析出的响应内容编码方式(备选编码方式)  |
| r.content           | HTTP响应内容的二进制形式。                      |

Requests库的异常

| 异常                      | 说明                                        |
| ------------------------- | ------------------------------------------- |
| requests.ConnectionError  | 网络连接错误异常，如DNS查询失败、拒绝连接等 |
| requests.HTTPError        | HTTP错误异常                                |
| requests.URLRequired      | URL缺失异常                                 |
| requests.TooManyRedirects | 超过最大重定向次数，产生重定向异常          |
| requests.ConnectTimeout   | 连接远程服务器超时异常                      |
| requests.Timeout          | 请求URL超时，产生超时异常                   |

r.raise_for_status() 如果不是200，产生异常 requests.HTTPError

异常处理通用框架：

```python
import requests
def getHTMLText(url):
    try:
        r = requests.get(url, timeout = 30)
        r.raise_for_status()  # 如果状态不是200，引发HTTPError异常
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "产生异常"
if __name__ == "__main__"
    url = "http://www.baidu.com"
    print(getHTMLText(url))
```

HTTP ，Hypertext Transfer Protocol 超文本传输协议，一个基于请求与响应模式的、无状态的应用层协议。采用URL作为定位网络资源的标识，URL格式如下:

```
http://host[:port][path]
```

host为合法的Internet主机域名或者IP地址；port为端口号，缺省为80；path请求资源的路径。

网络爬虫的尺寸

小规模，数据量小，爬取速度不敏感，主要是爬取网页级别，使用Requests库。

中规模，数据规模较大，爬取速度敏感，主要是爬取网站级别，使用Scrapy库。

大规模，搜索引擎爬取速度关键，主要是爬取全网信息，一般定制开发。

小规模爬虫占到了爬虫应用的90%以上。

网络爬虫的限制有两种方法，一是来源审查，主要是检查请求这的user-agent字段。二是发布公告，也就是robots。

Robots Exclusion Standard 网络爬虫排除标准，作用是网站告知网络爬虫那些页面可以抓取，那些不可以。

形式是在网站根目录下放置一个文本文件，robots.txt。比如

http://www.jd.com/robots.txt

```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

User-agent :那些访问者

Disallow:不允许访问的路径。

如果网站没有robots文件，那么说明允许无限制访问。

正则表达式 regular expression        regex        RE

正则表达式是用来简介表达一组字符串的表达式。

五个网站数据爬取实例：

Beautiful soup 美味汤，主要用来解析HTML和XML

```
pip3 install beautifulsoup4
```

```python
>>> import requests
>>> r = requests.get("http://python123.io/ws/demo.html")
>>> r.text
'<html><head><title>This is a python demo page</title></head>\r\n<body>\r\n<p class="title"><b>The demo python introduces several python courses.</b></p>\r\n<p class="course">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\r\n<a href="http://www.icourse163.org/course/BIT-268001" class="py1" id="link1">Basic Python</a> and <a href="http://www.icourse163.org/course/BIT-1001870001" class="py2" id="link2">Advanced Python</a>.</p>\r\n</body></html>'
>>> demo = r.text
>>> from bs4 import BeautifulSoup
>>> soup = BeautifulSoup(demo, "html.parser")
>>> print(soup.prettify())
```

soup = BeautifulSoup(demo, "html.parser")

解析函数第一个参数为数据，第二个为解析器。

Beautiful Soup库也叫beautifulsoup4，也叫bs4

from bs4 import BeautifulSoup

import bs4

Beautiful Soup库解析器

| 解析器           | 使用方法                        | 条件                 |
| ---------------- | ------------------------------- | -------------------- |
| bs4的HTML解析器  | BeautifulSoup(mk,'html.parser') | 安装bs4库            |
| lxml的HTML解析器 | BeautifulSoup(mk,'lxml')        | pip install lxml     |
| lxml的XML解析器  | BeautifulSoup(mk,'xml')         | pip install lxml     |
| html5lib的解析器 | BeautifulSoup(mk,'html5lib')    | pip install html5lib |

Beautiful Soup类的基本元素

| 基本元素        | 说明                                                         |
| --------------- | ------------------------------------------------------------ |
| Tab             | 标签，最基本的信息组织单元，分别使用<>和</>标明开头和结尾    |
| Name            | 标签的名字，< p >... < /p >的名字是p，格式：< tag >.name     |
| Attributes      | 标签的属性，字典形式组织，格式：< tag >.attrs                |
| NavigableString | 标签内非属性字符串，< >...< / >中的字符串，格式< tag >.string |
| Comment         | 标签内字符串的注释部分，一种搞特殊的Comment类型              |

```
>>> soup.title
<title>This is a python demo page</title>
>>> tag = soup.a
>>> tag
<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>
>>> soup.a.name  # a标签的名称
'a'
>>> soup.a.parent.name  # a标签的父标签的名称
'p'
>>> soup.a.parent.parent.name
'body'
>>> soup.a.parent.parent.parent.name
'html'
>>> soup.a.parent.parent.parent.parent.name
'[document]'
>>> tag.attrs  # 标签属性
{'class': ['py1'], 'href': 'http://www.icourse163.org/course/BIT-268001', 'id': 'link1'}
>>> soup.a.string # a标签的字符串
'Basic Python'
>>> 

```

Buautiful Soup中HTML标签的遍历方式

下行遍历

| 属性         | 说明                                                    |
| ------------ | ------------------------------------------------------- |
| .contents    | 子节点的列表，将< tag >所有儿子节点存入列表             |
| .children    | 子节点的迭代类型，与.contents类型，用于循环遍历儿子节点 |
| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历      |

```
for child in soup.body.chinlren  # 遍历儿子节点
    print(child)
```

上行遍历

| 属性     | 说明                                           |
| -------- | ---------------------------------------------- |
| .parent  | 节点的父亲标签                                 |
| .parents | 节点的先辈标签的迭代类型，用于循环遍历先辈节点 |

```python
for parent in soup.a.parents:
    if parent is None:
        print(parent)
    else:
        print(parent.name)
```

平行遍历

| 属性               | 说明                                                 |
| ------------------ | ---------------------------------------------------- |
| .next_sibling      | 返回按照HTML文本顺序的下一个平行节点标签             |
| .previous_sibling  | 返回按照HTML文本顺序的上一个平行节点标签             |
| .next_siblings     | 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签 |
| .previous_siblings | 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签 |

标签输的平行遍历必须是在同一个父亲节点下的各个节点。

在标签树中，平行遍历的下一个节点不一定是标签类型。

prettify()方法，在每一个标签后面增加了换行符号，这样打印出来的html文本更加友好和清晰。

bs4库把所有读入的文本都转换为了utf-8编码。

信息标记的三中形式

信息的标记

标记后的信息可形成信息组织结构，增加了信息维度，可用于通信、存储或展示，标记结构与信息一样有重要价值，有利于程序或人的利用和理解。

HTML(hyper text markup language)是www(World Wide Web)的信息组织方式。

能将声音，图像，视频，嵌入到文本中。HTML通过预定义的< >...< / >标签形式组织不同类型的信息。

信息标记目前国际工人的有三中形式分别是：XML、JSON，YAML

XML  extensible markup language

通过标签形式构建信息。注释为 <!----->

JSON  JavaScript Object Notation,

有类型的键值对 key:value  键值中字符串需要双引号，值可以有多个，使用“name":[“name1”,“name2”]，键值对可以嵌套使用，使用大括号{}表示

```
"key":"value"
"key":["value1","value2"]
"key":{"subkey":"subvalue"}
```

YAML YAML Ain't Markup Language

无类型的键值对组织数据，通过缩进表示所属关系，类似python，# 表示注释

XML实例

```xml
<person>
    <firstName>Jamie</firstName>
    <lastName>Yuan</lastName>
    <address>
        <streetAddr>槐安西路</streetAddr>
        <city>石家庄市</city>
        <zipcode>050000</zipcode>
    </address>
</person>
```

JSON实例

```json
{
    "firstName":"Jamie",
    "lastNAme" :"Yuan",
    "address"  :{
                  "streetAddr":"槐安西路"
        		  “city”      :"石家庄市"
        		  “zipcode”   :"050000"
                },
    "prof"     :["Computer System","Security"]
}
```

YAML实例

```yaml
firstName:Jamie
LastName :Yuan
address  :
    streetAddr:槐安西路
    city      :石家庄市
    zipcode   :050000
prof     :
-Computer System
-Security
```

三中信息标记形式的比较

XML最早的通用信息标记语言，可扩展性号，但是繁琐

JSON信息有类型，适合程序处理(js)，较XML简洁

YAML信息无类型，文本信息比例最高，可读性好

Internet上的信息交互主要使用XML

JSON在移动应用云端和节点的信息通信，无注释

YAML各类系统的配置文件，有注释易读

信息提取的一般方法：

方法一：完整解析信息的标记形式，在提取关键信息，需要标记解析器，例如bs4库的标签树遍历，优点是信息解析准确，缺点是提取过程繁琐，速度慢。

方法二：无视标记形式，直接搜索关键信息。对信息文本查找函数即可，优点是提取过程简洁，速度快，缺点是提取结果准确性与信息内容相关。

最好的方法是融合两种方法。

```python
>>> for link in soup.find_all('a'):
...     print(link.get('href'))
... 
http://www.icourse163.org/course/BIT-268001
http://www.icourse163.org/course/BIT-1001870001
```

<>.find_all(name, attrs,recursive,string,**kwargs)

返回一个列表类型，存储查找的结果。

name：对标签名称的检索字符串。

```python
soup.find_all('a')
soup.find_all(['a','b'])
for tag in soup.find_all(True):
    print(tag.name)
    
>>> import re
>>> for tag in soup.find_all(re.compile('b')):
...     print(tag.name)
```

attrs:对标签属性值的检索字符串，可标注属性检索。

```python
soup.find_all('p', 'course')
soup.find_all(id='link1')
import re
soup.find_all(id=re.compile('link'))
```

recursive:是否最子孙全部检索，默认为True。

```python
>>> soup.find_all('a')
[<a class="py1" href="http://www.icourse163.org/course/BIT-268001" id="link1">Basic Python</a>, <a class="py2" href="http://www.icourse163.org/course/BIT-1001870001" id="link2">Advanced Python</a>]
>>> soup.find_all('a', recursive=False)
[]
```

string:<>...</>标签中间的字符串域的检索字符串。

```python
>>> soup.find_all(string = "Basic Python")
['Basic Python']
>>> soup.find_all(string = re.compile("python"))
['This is a python demo page', 'The demo python introduces several python courses.']
```

由于find_all函数非常常用，所以存在一种简写形式

< tag >(..) 等价于 < tag >.fine_all(..)

soup(..) 等价于 soup.fine_all(..)

Beautiful Soup库的find_all方法有7个扩展方法：

| 方法                        | 说明                                                    |
| --------------------------- | ------------------------------------------------------- |
| <>.find()                   | 搜索且只返回一个结果，字符串类型，同.find_all()参数     |
| <>.find_parents()           | 在先辈节点中搜索，返回列表类型，同.find_all()参数       |
| <>.find_parent()            | 在先辈节点中返回一个结果，字符串类型，同.find()参数     |
| <>.find_next_siblings()     | 在后续平行节点中搜索，返回列表类型，同.find_all()参数   |
| <>.find_next_sibling()      | 在后续平行节点中返回一个结果，字符串类型，同.find()参数 |
| <>.find_previous_siblings() | 在前序平行节点中搜索，返回列表类型，同.find_all()参数   |
| <>.find_previous_sibling()  | 在前序平行节点中返回一个结果，字符串类型，同.find()参数 |

实例：中国大学排名实例

网址：

http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html

定向爬虫：仅对输入URL进行爬取，不扩展爬取。

程序见CrawUnivRanking.py

字符串匹配主要使用正则表达式。正则表达式的使用需要编译，也就是将符合正则表达式语法的字符串转换成正则表达式特征。

正则表达式语法，正则表达式是由字符和操作符构成。

```
P(Y|YT|YTH|YTHO)?N
```

正则表达式的常用操作符

| 操作符 | 说明                             | 实例                                    |
| ------ | -------------------------------- | --------------------------------------- |
| .      | 表示任何单个字符                 |                                         |
| []     | 字符集，对单个字符给出取值范围   | [abc]表示a、b、c，[a-z]表示a到z单个字符 |
| [^]    | 非字符集，对单个字符给出排除范围 | [^abc]表示非a或b或c的单个字符           |
| *      | 前一个字符0次或无限次扩展        | abc*表示ab,abc,abcc,abccc等             |
| +      | 前一个字符1次或无限次扩展        | abc+表示abc，abcc，abccc等              |
| ?      | 前一个字符0次或1次扩展           | abc？表示ab,bac                         |
| \|     | 左右表达式任意一个               | abc\|def表示abc、def                    |
| {m}    | 扩展前一个字符m次                | ab{2}c表示abbc                          |
| {m,n}  | 扩展前一个字符m至n次             | ab{1,2}c表示abc、abbc                   |
| ^      | 匹配字符串开头                   | ^abc表示abc且在一个字符串的开头         |
| $      | 匹配字符串结尾                   | abc$表示abc且在一个字符串的结尾         |
| ()     | 分组标记，内部只能使用\|操作符   | （abc)表示abc,(abc\|def)表示abc、def    |
| \d     | 数字，等价于[0-9]                |                                         |
| \w     | 单词字符，等价于[A-Za-z0-9]      |                                         |

^[A-Za-z]+$  由26个字母组成的字符串

^[A-Za-z0-9]+$  由26个字母和数字组成的字符串

^-?\d+$ 整数形式的字符串

^[0-9]*[1-9] [0-9] *$ 正整数形式的字符

[1-9]\d{5}中国境内邮政编码，6位

[\u4e00-\u9fa5]匹配中文字符

\d{3}-\d{8}|\d{4}-\d{7}  国内电话号码，010-68913536

匹配IP地址的正则表达式

IP地址字符串形式的正则表达式

\d+.\d+.\d+.\d+

\d{1,3}.\d{1,3}.\d{1,3}.\d{1,3}

以上两个表达式都不能精确的表示IP地址字符串。要想精确表达，需要分段考虑。

0-99：[1-9]?\d    100-199:1\d{2}  200-249:2[0-4]\d  250-255:25[0-5]

(([1-9]?\d|1\d{2}|2[0-4]\d|25[0-5]).){3}([1-9]?\d|1\d{2}|2[0-4]\d|25[0-5])

Re库是Python的标准库，主要用于字符串匹配，import re即可。

原生字符串raw string也就是不包含转义字符的字符串

r'[1-9]\d{5}'  

在python中\被解析为转义符号，如果需要使用\符号，需要 \ \表示。比较繁琐。这就引入了原生字符串。

Re库的主要功能函数

| 函数          | 说明                                                         |
| ------------- | ------------------------------------------------------------ |
| re.search()   | 在一个字符串中搜索匹配正则表达式的第一个位置，返回match对象  |
| re.match()    | 从一个字符串开始位置匹配正则表达式，返回match对象            |
| re.findall()  | 搜索字符串，以列表类型返回全部能匹配的字符子串               |
| re.split()    | 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型     |
| re.finditer() | 搜索字符串，反馈已给匹配结果的迭代类型，每个迭代元素是match对象 |
| re.sub()      | 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 |

re.search(pattern, string,flags=0)

pattern:正则表达式字符串或原生字符串表示

string： 待匹配的字符串

flags:正则表达式使用时的一些标记

| 常用标记            | 说明                                                         |
| ------------------- | ------------------------------------------------------------ |
| re.l  re.IGNORECASE | 忽略正则表达式的大小写                                       |
| re.M re.MULTILINE   | 正则表达式中的^操作符能将给定字符串的每一行当做配屏开始      |
| re.S re.DOTALL      | 正则表达式中的.操作符匹配所有字符串，默认匹配除了换行外的所有字符 |

例子：

```python
>>> import re
>>> match = re.search(r'[1-9]\d{5}', 'BIT 100081')
>>> if match:
...     print(match.group(0))

```

re.match(pattern, string,flags=0)

```python
>>> import re
>>> match = re.match(r'[1-9]\d{5}', 'BIT 100081')
>>> if match:
	match.group(0)

>>> match.group(0)
Traceback (most recent call last):
  File "<pyshell#5>", line 1, in <module>
    match.group(0)
AttributeError: 'NoneType' object has no attribute 'group'
>>> match = re.match(r'[1-9]\d{5}', '100081 BIT')
>>> if match:
	match.group(0)

	
'100081'
>>> 
```

re.findall(pattern, string,flags=0)

```python
>>> import re
>>> ls = re.findall(r'[1-9]\d{5}', 'BIT100081 TSU100084')
>>> ls
['100081', '100084']
>>> 
```

re.split(pattern, string,maxsplit=0,flags=0)

其他参数与之前的一行，增加了maxsplit参数，可以指定最大分割个数。

```python
>>> import re
>>> re.split(r'[1-9]\d{5}', 'BIT100081 TSU100084')
['BIT', ' TSU', '']
>>> re.split(r'[1-9]\d{5}', 'BIT100081 TSU100084', maxsplit = 1)
['BIT', ' TSU100084']
>>> 
```

re.finditer(pattern, string,flags=0)

```python
>>> import re
>>> for m in re.finditer(r'[1-9]\d{5}', 'BIT100081 TSU100084'):
	if m:
		print(m.group(0))

		
100081
100084
>>> 
```

re.sub(pattern, repl,string,count=0,flags=0)

在一个字符串中替换所有匹配的正则表达式的子串，返回替换后的字符串。

repl为替换字符串

count最大替换次数。

```python
>>> import re
>>> re.sub(r'[1-9]\d{5}',':zipcode', 'BIT100081 TSU100084')
'BIT:zipcode TSU:zipcode'
>>> 
```

Re库的另一种等价用法

```python
match = re.search(r'[1-9]\d{5}', 'BIT 100081')  #  函数式用法，一次性操作
```

下面是面向对象的方法。编译后多次操作

```python
pat = re.compile(r'[1-9]\d{5}')
rst = pat.search('BIT 100081')
```

regex = re.compile(pattern,flags=0)

将正则表达式的字符串形式编译成正则表达式对象。

正则表达式对象方法与正则表达式函数一致。

Re库的match对象

Match对象的属性

| 属性    | 说明                                 |
| ------- | ------------------------------------ |
| .string | 待匹配的文本                         |
| .re     | 匹配时使用的parttern对象(正则表达式) |
| .pos    | 正则表达式搜索文本的开始位置         |
| .endpos | 正则表达式搜索文本的结束位置         |

Match对象的方法

| 方法      | 说明                             |
| --------- | -------------------------------- |
| .group(0) | 获得匹配后的字符串               |
| .start()  | 匹配字符串在原始字符串的开始位置 |
| .end()    | 匹配字符串在原始字符串的结束位置 |
| .span()   | 返回（.start(), .ent())          |

```python
>>> import re
>>> m = re.search(r'[1-9]\d{5}', 'BIT100081 TSU100084')
>>> m.string
'BIT100081 TSU100084'
>>> m.re
re.compile('[1-9]\\d{5}')
>>> m.pos
0
>>> m.endpos
19
>>> m.group(0)
'100081'
>>> m.start()
3
>>> m.end()
9
>>> m.span()
(3, 9)
>>>
```

贪婪匹配和最小匹配

```python
>>> match = re.search(r'PY.*N', 'PYANBNCNDN')
>>> match.group(0)
'PYANBNCNDN'
```

同时匹配长短不同的多项，默认返回最长的一个。这个就是贪婪匹配。

```python
>>> match = re.search(r'PY.*?N', 'PYANBNCNDN')
>>> match.group(0)
'PYAN'
>>> 
```

最小匹配增加一个问号?即可。

最小匹配操作符

| 操作符 | 说明                                  |
| ------ | ------------------------------------- |
| *?     | 前一个字符0次或者无限次扩展，最小匹配 |
| +?     | 前一个字符1次或者无限次扩展，最小匹配 |
| ??     | 前一个字符0次或者1次扩展，最小匹配    |
| {m,n}? | 扩展前一个字符m至n次(含n次)，最小匹配 |

淘宝商品信息定向爬虫实例：

```python
import requests
import re


def getHTMLText(url):
    try:
        kv = {'user-agent': 'Mozilla/5.0'}
        r = requests.get(url, timeout=30, headers=kv)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return "err-get"


def parsePage(ilt,html):
    try:
        plt = re.findall(r'\"view_price\"\:\"[\d\.]*\"',html)
        tlt = re.findall(r'\"raw_title\"\:\".*?\"',html)
        for i in range(len(plt)):
            price = eval(plt[i].split(':')[1])
            title = eval(lt[i].split(':')[1])
            ilt.append([price, title])
            print("ilt len:%d",len(ilt))
    except:
        print("err-parse")


def printGoodsList(ilt):
    tplt = "{:4}\t{:8}\t{:16}"
    print(tplt.format("序号", "价格", "商品名称"))
    count = 0
    for g in ilt:
        count = count + 1
        print(tplt.format(count, g[0], g[1]))


def main():
    goods = '书包'
    depth = 2
    start_url = 'https://s.taobao.com/search?q=' + goods
    infoList = []
    for i in range(depth):
        try:
            url = start_url + '&s=' + str(44*i)
            html = getHTMLText(url)
            parsePage(infoList, html)

        except:
            continue
    printGoodsList(infoList)


main()

```

股票信息爬取实例

```python
import requests
from bs4 import BeautifulSoup
import traceback
import re


def getHTMLText(url, code='utf-8'):
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        r.encoding = code
        return r.text
    except:
        return ""


def getStockList(lst, stockURL):
    html = getHTMLText(stockURL, 'GB2312')
    soup = BeautifulSoup(html, 'html.parser')
    a = soup.find_all('a')
    for i in a:
        try:
            href = i.attrs['href']
            lst.append(re.findall(r"[s][hz]\d{6}", href)[0])
        except:
            continue


def getStockInfo(lst, stockURL, fpath):
    count = 0
    for stock in lst:
        url = stockURL + stock + ".html"
        html = getHTMLText(url)
        try:
            if html == "":
                continue
            infoDict = {}
            soup = BeautifulSoup(html, 'html.parser')
            stockInfo = soup.find('div', attrs={'class':'bets-name'})

            name = stockInfo.find_all(attrs={'class':'bets-name'})[0]
            infoDict.update({'股票名称':name.text.split()[0]})

            keyList = stockInfo.find_all('dt')
            valueList = stockInfo.findall('dd')
            for i in range(len(keyList)):
                key = keyList[i].text
                val = valueList[i].text
                infoDict[key] = val

            with open(fpath, 'a', encoding='utf-8') as f:
                f.write(str(infoDict)+'\n')
                count = count + 1
                print('\r当前速度：{:.2f}%'.format(count*100/len(lst)),end='')
        except:
            count = count + 1
            print('\r当前速度：{:.2f}%'.format(count * 100 / len(lst)), end='')
            continue


def main():
    stock_list_url = 'http://quote.eastmoney.com/stock'
    stock_inof_url = 'https://gupiao.baidu.com'
    output_file = '//home//angel_yy/StockInfo.txt'
    slist = []
    getStockList(slist, stock_list_url)
    getStockInfo(slist, stock_inof_url,output_file)


main()
```

Scrapy库

pip install scrapy进行安装

scrapy -h测试效果

Scrapy爬虫框架结构，5+2结构

有SPIDERS模块，ENGINE模块，SCHEDULER模块，DOWNLOADR模块，ITEM PIPELINES模块。

![Scrapy-about](Scrapy-about.png)

requests库好Scrapy比较：

相同点

两者都可以进行页面请求和爬取，python爬虫的两个重要技术路线。两者可用性都很好，文档丰富，入门简单；两者都没有处理js，提交表单，应对验证码等功能。

不同点

| request                  | Scrapy                     |
| ------------------------ | -------------------------- |
| 页面级爬虫               | 网站级爬虫                 |
| 功能库                   | 框架                       |
| 并发性考虑不足，性能较差 | 并发性号，性能较高         |
| 重点在于页面下载         | 重点在于爬虫结构           |
| 定制灵活                 | 一般定制灵活，深度定制困难 |
| 上手十分简单             | 入门稍难                   |

选用那种技术路线开发爬虫

非常小的需求，requests库

不太小的需求，Scrapy框架
持续，不间断，周期性，爬取信息。

Scrapy常用命令格式

scrapy  < command >[options] [args]

 Scrapy常用命令

| 命令         | 说明               | 格式                                         |
| ------------ | ------------------ | -------------------------------------------- |
| startproject | 创建一个新工程     | scrapy startproject < name > [dir]           |
| genspider    | 创建一个爬虫       | scrapy genspider [options]< name >< domain > |
| settings     | 获得爬虫配置信息   | scrapy settings [options]                    |
| crawl        | 运行一个爬虫       | scrapycrawl < spider >                       |
| list         | 列出工程中所有爬虫 | scrapy list                                  |
| shell        | 启动URL调试命令行  | scrapy shell [url]                           |

为什么Scrapy采用命令行创建和运行爬虫？

命令行更容易自动化，适合脚本控制。本质上，Scrapy是给程序员用的，功能更重要。

```shell
scrapy startproject python123demo
```

第一步--生成工程目录

python123demo/     ----> 外层目录

​    scrapy.cfg               ---->部署Scrapy爬虫的配置文件

​    python123demo/  ---->Scrapy框架的用户自定义python代码

​         __ init __ .py       ---->初始化脚本

​        items.py             ---->Items代码模板(继承类)

​        middlewares.py ---->Middlewares代码模板(继承类)

​        pipelines.py       ---->Pipelines代码模板(继承类)

​        settings.py         ---->Scrapy代码模块(继承类)

​        spiders/             ---->Spiders代码模板目录(继承类)

​            __ init __.py    ---->初始化文件，无需修改

​            __ pycach __/ ---->缓存目录，无需修改

​           demo.py         ---->使用生成爬虫命令后新增文件

第二步--生成爬虫：

```shell
scrapy genspider demo python123.io
```

demo.py内容：

```python
# -*- coding: utf-8 -*-
import scrapy

class DemoSpider(scrapy.Spider):
    name = "demo"
    allowed_domains = ["python123.io"]
    start_urls = ['http://python123.io/']
    
    def parse(self, response):
        pass
```

第三步-- 配置产生的spiders爬虫

修改demo.py文件

```python
# -*- coding: utf-8 -*-
import scrapy

class DemoSpider(scrapy.Spider):
    name = "demo"
    #allowed_domains = ["python123.io"]
    start_urls = ['http://python123.io/ws/demo.html']
    
    def parse(self, response):
        fname = response.url.split('/')[-1]
        with open(fname ,'wb') as f:
            f.wirte(response.body)
        self.log('Saved file %s.' % name)
```

经过改造后，demo.html就可以保存到本地了。

第四步-- 执行爬虫

```shell
scrapy crawl demo
```

yield关键字，生成器，是一个不断产生值的函数。包含yield语句的函数就是一个生成器。生成器每一次产生一个值(yield语句)，函数被冻结，被唤醒后再次产生一个值。

生成器实例

```python
>>> def(n):
        for i in range(n):
            yield i**2
```

生成器通常与循环搭配使用。

为何要有生成器？

生成器比一次列出所有内容的优势：节省存储空间，相应更迅速，使用更灵活。

```python
# -*- coding: utf-8 -*-
import scrapy

class DemoSpider(scrapy.Spider):
    name = "demo"
    
    def start_requests(Self):
        urls = [
                   'http://python123.io/ws/demo.html'
               ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)
    
    def parse(self, response):
        fname = response.url.split('/')[-1]
        with open(fname ,'wb') as f:
            f.wirte(response.body)
        self.log('Saved file %s.' % name)
```

Scrapy爬虫的使用步骤

步骤1：创建一个工程和Spider模板

步骤2：编写Spider

步骤3：编写Item Pipeline

步骤4：优化配置策略

Scrapy爬虫的数据类型

Request类、Response类、Item类

Request类    class scrapy.http.Request() 

Request对象表示一个HTTP请求，由Spider生成，由Downloader执行。

| 属性或方法 | 说明                                               |
| ---------- | -------------------------------------------------- |
| .url       | Request对应的请求URL地址。                         |
| .method    | 对应的请求方法，‘GET’‘POST’等                      |
| .headers   | 字典类型风格的请求头                               |
| .body      | 请求内容主体，字符串类型                           |
| .meta      | 用户添加的扩展信息，在Scrapy内部模块间传递信息使用 |
| .copy()    | 复制该请求                                         |

Response类

class scrapy.http.Response()

Response对象表示一个HTTP响应，由Downlaoder生成，有Spider处理。

Response类型

| 属性或方法 | 说明                               |
| ---------- | ---------------------------------- |
| .url       | Response嘴硬的URL地址              |
| .status    | HTTP状态码，默认200                |
| .headers   | Response对应的头部信息             |
| .body      | Response对象的内容信息，字符串类型 |
| .flags     | 一组标记                           |
| .request   | 产生Response类型对应的Request对象  |
| .copy()    | 复制该响应                         |

Item类

class scrapy.item.Item()

Item对象表示一个从HTML页面纵提取的信息内容。由Spider生成，由Item Pipeline处理。类似字典，可以按照字典类型操作。

Scrapy爬虫的信息提取方法

Scrapy爬虫支持多种HTML信息提取方法

- Beautiful Soup
- lxml
- re
- XPath Selector
- CSS Selector

CSS Selector由W3C组织维护并规范

```python
<HTML>.css('a::attr(href)').extract()
```

根据标签名，标签属性提取HTML中的信息。

股票数据Scrapy爬虫实例：

数据网站确定

获取股票列表：

东方财富网： http://quote.eastmoney.com/stocklist.html

获取个股信息：

百度股票：https://gupiao.baidu.com/stock/

单个股票：https://gupiao.baidu.com/stock/sz002439.html

步骤

1：建立工程和Spider模板

```shell
\> scrapy startproject BaiduStocks
\> cd BaiduStocks
\> scrapy genspider stocks baidu.com
```

2：编写Spider

进一步修改spiders/stocks.py文件，修改对返回页面的处理，修改对新增URL爬取请求的处理

```python
# -*- coding: utf-8 -*-
import scrapy
import re

class StocksSpider(scrapy.Spider):
    name = "stocks"
    start_urls = ['http://quote.eastmoney.com/stocklist.html']
    
    def parse(self, response):
        for href in response.css('a::attr(href)').extract():
            try:
                stock = re.findall(r"[s][hz]\d{6}",href)[0]
                url = 'https://gupiao.baidu.com/stock/' + stock + '.html'
                yield scrapy.Request(url, callback=self.parse_stock)
            except:
                continue
    def parse_stock(self, response):
        infoDict = {}
        stockInfo = response.css('.stock-bets')
        name = stockInfo.css('.bets-name').extract()[0]
        keylist = stockInfo.css('dt').extract()
        valueList = stockInfo.css('dd').extract()
        for i in range(len(keyList)):
            key = re.findall(r'>.*</dt>',keyList[i])[0][1:-5]
            try:
                val = re.findall(r'\d+\.?.*</dd>',valueList[i])[0][0:-5]
            except:
                val = '--'
            infoDict[key] = val
        infoDict.update(
                      {'股票名称': re.findall('\s.*\(',name)[0].split()[0]+\
                       re.findall('\>.*\<', name)[0][1:-1]})
        yield infoDict
```

3：编写Item Pipelines

配置pipelines.py文件，定义对派去项(Scraped Item)的处理，配置ITEM_PIPELINES选项

```python
# -*- coding: utf-8 -*-
# Define your item pipelines here
#
# Don't forget to add your pipeline to zhe ITEM_PIPELINES settir
# See: http://doc.scrapy.org/en/lotest/topics/item-pipeline.html


class BaidutocksPipeline(object):
    def process_item(self, item, spider):
        return item

    
class BaidustocksInfoPipeline(object):
    def open_spider(self, spider):
        self.f = open ('BaiduStockInfo.txt', 'w')
        
    def close_spider(self, spider):
        self.f.close()
        
    def process_item(self,item,spider):
        try:
            line = str(dict(item)) + '\n'
            self.f.write(line)
        except:
            pass
        return item

```

打开settings.py

找到ITEM_PIPELINES项,添加自己编写的类到该列表中即可。

 所有代码编写完成后，执行程序

```shell
\>scrapy crawl stocks
```

Scrapy配置并发连接选项 settings.py文件中

| 选项                           | 说明                                         |
| ------------------------------ | -------------------------------------------- |
| CONCURRENT_REQUESTS            | Downlaoder最大并发请求下载数量，默认32       |
| CONCURRENT_ITEMS               | Item Pipeline最大并发ITEM处理数量，默认100   |
| CONCURRENT_REQUESTS_PER_DOMAIN | 每个目标域名最大的并发请求数量，默认8        |
| CONCURRENT_REQUESTS_PER_IP     | 每个目标IP最大的并发请求数量，默认0，非0有效 |

